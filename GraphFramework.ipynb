{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/patrickzel/flight-delay-and-cancellation-dataset-2019-2023?dataset_version_number=7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140M/140M [00:03<00:00, 46.2MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/katarinadvornak/.cache/kagglehub/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023/versions/7\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"patrickzel/flight-delay-and-cancellation-dataset-2019-2023\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/01 14:32:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.1\n",
      "Spark Session Initialized.\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports and Environment Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark Session (Big Data Safe)\n",
    "# This MUST run before any command that uses the 'spark' object.\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"FlightDelayGraphAnalysis\")\n",
    "    .getOrCreate())\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(\"Spark Session Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Successfully Ingested. Total Records: 3,000,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pyspark_data_path = \"/Users/katarinadvornak/.cache/kagglehub/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023/versions/7/flights_sample_3m.csv\"\n",
    "\n",
    "try:\n",
    "    raw_flights_df = (spark.read.csv(\n",
    "        pyspark_data_path,\n",
    "        header=True,\n",
    "        schema=final_flight_schema, \n",
    "        ignoreLeadingWhiteSpace=True)\n",
    "    )\n",
    "\n",
    "    print(f\"\\n‚úÖ Data Successfully Ingested. Total Records: {raw_flights_df.count():,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Ingestion Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting directory: /Users/katarinadvornak/.cache/kagglehub/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023/versions/7\n",
      "\n",
      "Found 1 data files. First file: /Users/katarinadvornak/.cache/kagglehub/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023/versions/7/flights_sample_3m.csv\n",
      "PySpark ingestion path (FIXED): /Users/katarinadvornak/.cache/kagglehub/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023/versions/7/flights_sample_3m.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total Records Loaded (Estimated): 3,000,000\n",
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- AIRLINE_CODE: string (nullable = true)\n",
      " |-- DOT_CODE: integer (nullable = true)\n",
      " |-- FL_NUMBER: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY: string (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- AIR_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- DELAY_DUE_CARRIER: double (nullable = true)\n",
      " |-- DELAY_DUE_WEATHER: double (nullable = true)\n",
      " |-- DELAY_DUE_NAS: double (nullable = true)\n",
      " |-- DELAY_DUE_SECURITY: double (nullable = true)\n",
      " |-- DELAY_DUE_LATE_AIRCRAFT: double (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_TIME: double (nullable = true)\n",
      " |-- TAXI_OUT: double (nullable = true)\n",
      " |-- WHEELS_OFF: double (nullable = true)\n",
      " |-- WHEELS_ON: double (nullable = true)\n",
      " |-- TAXI_IN: double (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_TIME: double (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- ELAPSED_TIME: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Inspect the downloaded directory to find the actual data files\n",
    "# The 'path' variable holds the local directory where the data was downloaded.\n",
    "print(f\"Inspecting directory: {path}\")\n",
    "\n",
    "# Assuming the data files are CSVs (e.g., 'flights.csv' or multiple monthly files)\n",
    "# Adjust the pattern if your files are named differently (e.g., '*.parquet' or 'flight_data_*.csv')\n",
    "file_pattern = os.path.join(path, \"*.csv\")\n",
    "data_files = glob.glob(file_pattern)\n",
    "\n",
    "if not data_files:\n",
    "    print(\"\\nüö® ERROR: No CSV files found in the downloaded directory.\")\n",
    "    print(\"Please check the directory structure or the file pattern.\")\n",
    "    # Exit or raise an error if critical files are missing\n",
    "else:\n",
    "    print(f\"\\nFound {len(data_files)} data files. First file: {data_files[0]}\")\n",
    "    \n",
    "    # 2. Define the PySpark Ingestion Path\n",
    "    # --- CRITICAL FIX: OVERRIDE WILDCARD WITH EXACT PATH ---\n",
    "    # Since we know the file is 'flights_sample_3m.csv', we must use the exact path found by glob.\n",
    "    pyspark_data_path = data_files[0] # Use the first (and only) file found by glob\n",
    "    \n",
    "    # Original logic (now overridden but left for context):\n",
    "    # pyspark_data_path = file_pattern\n",
    "    \n",
    "    print(f\"PySpark ingestion path (FIXED): {pyspark_data_path}\")\n",
    "    \n",
    "    # --- 3. Data Ingestion using Pre-defined Schema (Big Data Safe) ---\n",
    "    # We rely on the 'spark' session and 'final_flight_schema' (assuming it's defined correctly)\n",
    "    \n",
    "    try:\n",
    "        # NOTE: Ensure 'final_flight_schema' is used instead of 'flight_schema' if you updated the name.\n",
    "        raw_flights_df = (spark.read.csv(\n",
    "            pyspark_data_path, # Now using the fixed path\n",
    "            header=True,\n",
    "            schema=final_flight_schema, # Use the correct, fixed schema\n",
    "            ignoreLeadingWhiteSpace=True)\n",
    "        )\n",
    "\n",
    "        # 4. Initial Inspection\n",
    "        print(f\"\\n‚úÖ Total Records Loaded (Estimated): {raw_flights_df.count():,}\")\n",
    "        raw_flights_df.printSchema()\n",
    "        # raw_flights_df.limit(5).toPandas() # Uncomment if you want to see a preview\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during PySpark Ingestion. Check column names/types against the schema.\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 14:46:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: FL_DATE, AIRLINE, DOT_CODE, ORIGIN, DEST, DEST_CITY, TAXI_OUT\n",
      " Schema: FL_DATE, AIRLINE_CODE, ORIGIN, DEST, DEP_DELAY, ARR_DELAY, DISTANCE\n",
      "Expected: AIRLINE_CODE but found: AIRLINE\n",
      "CSV file: file:///Users/katarinadvornak/.cache/kagglehub/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023/versions/7/flights_sample_3m.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows in raw_flights_df: 3,000,000\n",
      "\n",
      "--- DataFrame Schema (Data Types) ---\n",
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- AIRLINE_CODE: string (nullable = true)\n",
      " |-- DOT_CODE: integer (nullable = true)\n",
      " |-- FL_NUMBER: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY: string (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- AIR_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- DELAY_DUE_CARRIER: double (nullable = true)\n",
      " |-- DELAY_DUE_WEATHER: double (nullable = true)\n",
      " |-- DELAY_DUE_NAS: double (nullable = true)\n",
      " |-- DELAY_DUE_SECURITY: double (nullable = true)\n",
      " |-- DELAY_DUE_LATE_AIRCRAFT: double (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_TIME: double (nullable = true)\n",
      " |-- TAXI_OUT: double (nullable = true)\n",
      " |-- WHEELS_OFF: double (nullable = true)\n",
      " |-- WHEELS_ON: double (nullable = true)\n",
      " |-- TAXI_IN: double (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_TIME: double (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- ELAPSED_TIME: double (nullable = true)\n",
      "\n",
      "\n",
      "--- Head Object (First 5 Rows) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>AIRLINE_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-09</td>\n",
       "      <td>19977</td>\n",
       "      <td>FLL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>19790</td>\n",
       "      <td>MSP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Delta Air Lines Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>19977</td>\n",
       "      <td>DEN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>19790</td>\n",
       "      <td>MSP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Delta Air Lines Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>20416</td>\n",
       "      <td>MCO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Spirit Air Lines</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FL_DATE ORIGIN DEST  DEP_DELAY  ARR_DELAY  DISTANCE  \\\n",
       "0  2019-01-09  19977  FLL        NaN        NaN      19.0   \n",
       "1  2022-11-19  19790  MSP        NaN        NaN       9.0   \n",
       "2  2022-07-22  19977  DEN        NaN        NaN      20.0   \n",
       "3  2023-03-06  19790  MSP        NaN        NaN      27.0   \n",
       "4  2020-02-23  20416  MCO        NaN        NaN      15.0   \n",
       "\n",
       "            AIRLINE_CODE  \n",
       "0  United Air Lines Inc.  \n",
       "1   Delta Air Lines Inc.  \n",
       "2  United Air Lines Inc.  \n",
       "3   Delta Air Lines Inc.  \n",
       "4       Spirit Air Lines  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Total Row Count (Volume Check)\n",
    "total_rows = raw_flights_df.count()\n",
    "print(f\"Total Rows in raw_flights_df: {total_rows:,}\")\n",
    "\n",
    "# 2. Schema Check (Data Types)\n",
    "# Verifies the types were correctly applied by the final_flight_schema.\n",
    "print(\"\\n--- DataFrame Schema (Data Types) ---\")\n",
    "raw_flights_df.printSchema()\n",
    "\n",
    "# 3. Head Object (First 5 Rows)\n",
    "# Displays the first 5 rows for visual verification of data integrity.\n",
    "print(\"\\n--- Head Object (First 5 Rows) ---\")\n",
    "# Only select key columns for a cleaner display:\n",
    "key_columns = [\"FL_DATE\", \"ORIGIN\", \"DEST\", \"DEP_DELAY\", \"ARR_DELAY\", \"DISTANCE\", \"AIRLINE_CODE\"]\n",
    "\n",
    "raw_flights_df.select(*key_columns).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Flight Schema Defined based on the data dictionary.\n"
     ]
    }
   ],
   "source": [
    "## üìù Define Final Schema (Big Data Safe Practice)\n",
    "\n",
    "# Define the definitive schema based on the provided dictionary (\"Updated Header\")\n",
    "flight_schema = StructType([\n",
    "    # Key Identifiers\n",
    "    StructField(\"FL_DATE\", StringType(), True),         \n",
    "    StructField(\"AIRLINE\", StringType(), True),    # Used to be OP_UNIQUE_CARRIER\n",
    "    StructField(\"DOT_CODE\", IntegerType(), True),\n",
    "    StructField(\"FL_NUMBER\", IntegerType(), True),\n",
    "    \n",
    "    # Graph Vertices\n",
    "    StructField(\"ORIGIN\", StringType(), True),           # Origin Airport Code (CRUCIAL)\n",
    "    StructField(\"ORIGIN_CITY\", StringType(), True),      # Useful property for Vertices\n",
    "    StructField(\"DEST\", StringType(), True),             # Destination Airport Code (CRUCIAL)\n",
    "    StructField(\"DEST_CITY\", StringType(), True),        # Useful property for Vertices\n",
    "    \n",
    "    # Time and Delay Metrics (Using DoubleType for flexibility with nulls and floats)\n",
    "    StructField(\"DEP_DELAY\", DoubleType(), True),\n",
    "    StructField(\"ARR_DELAY\", DoubleType(), True),      # Key analysis column\n",
    "    StructField(\"CANCELLED\", DoubleType(), True),\n",
    "    StructField(\"DIVERTED\", DoubleType(), True),\n",
    "    StructField(\"AIR_TIME\", DoubleType(), True),       # Key Edge Property\n",
    "    StructField(\"DISTANCE\", DoubleType(), True),       # Key Edge Property\n",
    "    \n",
    "    # Detailed Delay Attributions\n",
    "    StructField(\"DELAY_DUE_CARRIER\", DoubleType(), True),\n",
    "    StructField(\"DELAY_DUE_WEATHER\", DoubleType(), True),\n",
    "    StructField(\"DELAY_DUE_NAS\", DoubleType(), True),\n",
    "    StructField(\"DELAY_DUE_SECURITY\", DoubleType(), True),\n",
    "    StructField(\"DELAY_DUE_LATE_AIRCRAFT\", DoubleType(), True),\n",
    "    \n",
    "    # Include other columns needed for ETL/cleaning\n",
    "    StructField(\"CRS_DEP_TIME\", IntegerType(), True),\n",
    "    StructField(\"DEP_TIME\", DoubleType(), True),\n",
    "    StructField(\"TAXI_OUT\", DoubleType(), True),\n",
    "    StructField(\"WHEELS_OFF\", DoubleType(), True),\n",
    "    StructField(\"WHEELS_ON\", DoubleType(), True),\n",
    "    StructField(\"TAXI_IN\", DoubleType(), True),\n",
    "    StructField(\"CRS_ARR_TIME\", IntegerType(), True),\n",
    "    StructField(\"ARR_TIME\", DoubleType(), True),\n",
    "    StructField(\"CANCELLATION_CODE\", StringType(), True),\n",
    "    StructField(\"CRS_ELAPSED_TIME\", DoubleType(), True),\n",
    "    StructField(\"ELAPSED_TIME\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "print(\"Final Flight Schema Defined based on the data dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 14:51:33 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/12/01 14:51:34 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 32, schema size: 30\n",
      "CSV file: file:///Users/katarinadvornak/.cache/kagglehub/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023/versions/7/flights_sample_3m.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records remaining after critical cleaning: 0\n",
      "\n",
      "Edges DataFrame (Preview):\n",
      "+---+---+--------+---------+----------+-------+----+\n",
      "|src|dst|distance|arr_delay|is_delayed|carrier|date|\n",
      "+---+---+--------+---------+----------+-------+----+\n",
      "+---+---+--------+---------+----------+-------+----+\n",
      "\n",
      "\n",
      "Total Unique Airports (Vertices): 0\n",
      "Vertices DataFrame (Preview):\n",
      "+---+----+\n",
      "|id |city|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n",
      "\n",
      "--- Instantiating GraphFrame ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:146: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o212.loadClass.\n: java.lang.ClassNotFoundException: org.graphframes.GraphFramePythonAPI\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m# --- 3. Instantiate the GraphFrame ---\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m--- Instantiating GraphFrame ---\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m g \u001b[39m=\u001b[39m GraphFrame(vertices_df, edges_df)\n\u001b[1;32m     66\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGraphFrame created with \u001b[39m\u001b[39m{\u001b[39;00mg\u001b[39m.\u001b[39mvertices\u001b[39m.\u001b[39mcount()\u001b[39m:\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Vertices and \u001b[39m\u001b[39m{\u001b[39;00mg\u001b[39m.\u001b[39medges\u001b[39m.\u001b[39mcount()\u001b[39m:\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Edges.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGraph Ready for Analysis!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphframes/graphframe.py:65\u001b[0m, in \u001b[0;36mGraphFrame.__init__\u001b[0;34m(self, v, e)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sqlContext \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39msql_ctx\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sqlContext\u001b[39m.\u001b[39m_sc\n\u001b[0;32m---> 65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm_gf_api \u001b[39m=\u001b[39m _java_api(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc)\n\u001b[1;32m     67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mID \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm_gf_api\u001b[39m.\u001b[39mID()\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSRC \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm_gf_api\u001b[39m.\u001b[39mSRC()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/graphframes/graphframe.py:38\u001b[0m, in \u001b[0;36m_java_api\u001b[0;34m(jsc)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_java_api\u001b[39m(jsc):\n\u001b[1;32m     37\u001b[0m     javaClassName \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39morg.graphframes.GraphFramePythonAPI\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m jsc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mThread\u001b[39m.\u001b[39mcurrentThread()\u001b[39m.\u001b[39mgetContextClassLoader()\u001b[39m.\u001b[39mloadClass(javaClassName) \\\n\u001b[1;32m     39\u001b[0m             \u001b[39m.\u001b[39mnewInstance()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1363\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1365\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpy4j\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotocol\u001b[39;00m \u001b[39mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    283\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o212.loadClass.\n: java.lang.ClassNotFoundException: org.graphframes.GraphFramePythonAPI\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "## ‚öôÔ∏è Cell 11: Cleaning, Feature Engineering, and Graph Setup\n",
    "\n",
    "from pyspark.sql.functions import when, col, lit, count, desc\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# --- 1. Data Cleaning and Feature Engineering (Preparing Edges) ---\n",
    "\n",
    "# Filter out non-essential nulls and ill-defined data points.\n",
    "# We must have ORIGIN/DEST, ARR_DELAY, and DISTANCE for graph analysis.\n",
    "cleaned_flights_df = (raw_flights_df\n",
    "    .filter(col(\"ORIGIN\").isNotNull())\n",
    "    .filter(col(\"DEST\").isNotNull())\n",
    "    .filter(col(\"ARR_DELAY\").isNotNull()) # We must know the delay to analyze the edge\n",
    "    .filter(col(\"DISTANCE\") > 0)          # Distance must be positive\n",
    "    .cache() # Cache this key intermediate result! (Big Data Safe Practice for reuse)\n",
    ")\n",
    "print(f\"Records remaining after critical cleaning: {cleaned_flights_df.count():,}\")\n",
    "\n",
    "# Create the final Edge DataFrame with necessary GraphFrames column names and features\n",
    "edges_df = (cleaned_flights_df\n",
    "    .select(\n",
    "        col(\"ORIGIN\").alias(\"src\"),    # Source airport (MANDATORY)\n",
    "        col(\"DEST\").alias(\"dst\"),      # Destination airport (MANDATORY)\n",
    "        col(\"DISTANCE\").alias(\"distance\"),\n",
    "        col(\"ARR_DELAY\").alias(\"arr_delay\"),\n",
    "        \n",
    "        # FEATURE ENGINEERING: Create the is_delayed flag (Edge Property)\n",
    "        # This is a critical edge property for motif finding and analysis.\n",
    "        when(col(\"ARR_DELAY\") > 15, 1).otherwise(0).alias(\"is_delayed\"),\n",
    "        \n",
    "        col(\"AIRLINE_CODE\").alias(\"carrier\"), \n",
    "        col(\"FL_DATE\").alias(\"date\")\n",
    "    )\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(\"\\nEdges DataFrame (Preview):\")\n",
    "edges_df.limit(3).show()\n",
    "\n",
    "\n",
    "# --- 2. Create the Vertex DataFrame (Nodes) ---\n",
    "# Vertices must have an 'id' column. We include city information as a property.\n",
    "all_origins = cleaned_flights_df.select(\n",
    "    col(\"ORIGIN\").alias(\"id\"), \n",
    "    col(\"ORIGIN_CITY\").alias(\"city\")\n",
    ").distinct()\n",
    "\n",
    "all_destinations = cleaned_flights_df.select(\n",
    "    col(\"DEST\").alias(\"id\"), \n",
    "    col(\"DEST_CITY\").alias(\"city\")\n",
    ").distinct()\n",
    "\n",
    "# Union all origins and destinations to get all unique airports (Big Data Safe Union)\n",
    "# Note: UnionByName handles cases where columns might be slightly misaligned (though here they are clean).\n",
    "vertices_df = all_origins.unionByName(all_destinations).distinct().cache()\n",
    "\n",
    "print(f\"\\nTotal Unique Airports (Vertices): {vertices_df.count():,}\")\n",
    "print(\"Vertices DataFrame (Preview):\")\n",
    "vertices_df.limit(5).show(truncate=False)\n",
    "\n",
    "\n",
    "# --- 3. Instantiate the GraphFrame ---\n",
    "print(\"\\n--- Instantiating GraphFrame ---\")\n",
    "g = GraphFrame(vertices_df, edges_df)\n",
    "\n",
    "print(f\"GraphFrame created with {g.vertices.count():,} Vertices and {g.edges.count():,} Edges.\")\n",
    "print(\"Graph Ready for Analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_course_env)",
   "language": "python",
   "name": "ai_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
