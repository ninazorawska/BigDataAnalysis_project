{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43929742-98e2-4b76-8558-ec4ff19bd88e",
   "metadata": {},
   "source": [
    "**DATAFRAMES & SparkSQL** \n",
    "\n",
    "   In this file we want to present our knowledge and ability to work with DataFrames and SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857020bf-0145-4f72-bd52-04ece39f0119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/ninazorawska/Desktop/~bda/ai_course_env/lib/python3.13/site-packages (4.0.1)\n",
      "Requirement already satisfied: findspark in /Users/ninazorawska/Desktop/~bda/ai_course_env/lib/python3.13/site-packages (2.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /Users/ninazorawska/Desktop/~bda/ai_course_env/lib/python3.13/site-packages (from pyspark) (0.10.9.9)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0791bfaa-d732-4955-bbcd-4f932907fc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/11 12:16:06 WARN Utils: Your hostname, Ninas-macbook.local, resolves to a loopback address: 127.0.0.1; using 10.10.4.47 instead (on interface en0)\n",
      "25/12/11 12:16:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/11 12:16:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.10.4.47:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MySparkApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10c2b0830>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34340c61-3700-453d-8c02-aeaf7b37b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_df = spark.read.csv(\"winequality-red.csv\", header=True, inferSchema=True, sep=';')\n",
    "white_df = spark.read.csv(\"winequality-white.csv\", header=True, inferSchema=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae070cca-3b15-412b-be67-2203c10b3c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "|fixed acidity|volatile acidity|citric acid|residual sugar|chlorides|free sulfur dioxide|total sulfur dioxide|density|  pH|sulphates|alcohol|quality|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|\n",
      "|          7.8|            0.88|        0.0|           2.6|    0.098|               25.0|                67.0| 0.9968| 3.2|     0.68|    9.8|      5|\n",
      "|          7.8|            0.76|       0.04|           2.3|    0.092|               15.0|                54.0|  0.997|3.26|     0.65|    9.8|      5|\n",
      "|         11.2|            0.28|       0.56|           1.9|    0.075|               17.0|                60.0|  0.998|3.16|     0.58|    9.8|      6|\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "only showing top 5 rows\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "|fixed acidity|volatile acidity|citric acid|residual sugar|chlorides|free sulfur dioxide|total sulfur dioxide|density|  pH|sulphates|alcohol|quality|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "|          7.0|            0.27|       0.36|          20.7|    0.045|               45.0|               170.0|  1.001| 3.0|     0.45|    8.8|      6|\n",
      "|          6.3|             0.3|       0.34|           1.6|    0.049|               14.0|               132.0|  0.994| 3.3|     0.49|    9.5|      6|\n",
      "|          8.1|            0.28|        0.4|           6.9|     0.05|               30.0|                97.0| 0.9951|3.26|     0.44|   10.1|      6|\n",
      "|          7.2|            0.23|       0.32|           8.5|    0.058|               47.0|               186.0| 0.9956|3.19|      0.4|    9.9|      6|\n",
      "|          7.2|            0.23|       0.32|           8.5|    0.058|               47.0|               186.0| 0.9956|3.19|      0.4|    9.9|      6|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- fixed acidity: double (nullable = true)\n",
      " |-- volatile acidity: double (nullable = true)\n",
      " |-- citric acid: double (nullable = true)\n",
      " |-- residual sugar: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- free sulfur dioxide: double (nullable = true)\n",
      " |-- total sulfur dioxide: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- quality: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "red_df.show(5)\n",
    "white_df.show(5)\n",
    "red_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7194e99c-e933-4541-98e6-857188a9ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "red_df = red_df.withColumn(\"wine_type\", lit(\"red\"))\n",
    "white_df = white_df.withColumn(\"wine_type\", lit(\"white\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69772734-5567-4700-9bc1-b55d313bdd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+---------+\n",
      "|fixed acidity|volatile acidity|citric acid|residual sugar|chlorides|free sulfur dioxide|total sulfur dioxide|density|  pH|sulphates|alcohol|quality|wine_type|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+---------+\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|      red|\n",
      "|          7.8|            0.88|        0.0|           2.6|    0.098|               25.0|                67.0| 0.9968| 3.2|     0.68|    9.8|      5|      red|\n",
      "|          7.8|            0.76|       0.04|           2.3|    0.092|               15.0|                54.0|  0.997|3.26|     0.65|    9.8|      5|      red|\n",
      "|         11.2|            0.28|       0.56|           1.9|    0.075|               17.0|                60.0|  0.998|3.16|     0.58|    9.8|      6|      red|\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|      red|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "wine_df = red_df.union(white_df)\n",
    "wine_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b449c09-9a95-4f0c-bddd-b4b13fae52ab",
   "metadata": {},
   "source": [
    "Basic DataFrame operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b660bb2-b825-40e5-9a05-98ad89415379",
   "metadata": {},
   "source": [
    "1. Counting rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c70f9631-bc66-473a-ab76-bd75a1a7c0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6497"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de29a3-ed0e-4568-9764-0e1aac412f6b",
   "metadata": {},
   "source": [
    "2. Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e44831c-9e8a-4d3f-b177-2cd92cc3f6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/11 12:16:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+-------------------+------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+------------------+------------------+---------+\n",
      "|summary|    fixed acidity|   volatile acidity|        citric acid|    residual sugar|          chlorides|free sulfur dioxide|total sulfur dioxide|             density|                 pH|          sulphates|           alcohol|           quality|wine_type|\n",
      "+-------+-----------------+-------------------+-------------------+------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+------------------+------------------+---------+\n",
      "|  count|             6497|               6497|               6497|              6497|               6497|               6497|                6497|                6497|               6497|               6497|              6497|              6497|     6497|\n",
      "|   mean| 7.21530706479913|0.33966599969216654|0.31863321533014727|  5.44323533938742|0.05603386178236105| 30.525319378174544|   115.7445744189626|  0.9946966338309939| 3.2185008465445577| 0.5312682776666166|10.491800831152862| 5.818377712790519|     NULL|\n",
      "| stddev|1.296433757799814|0.16463647408467896|0.14531786489759146|4.7578037431474165|0.03503360137245908|   17.7493997720025|   56.52185452263029|0.002998673003719...|0.16078720210398784|0.14880587361449052|1.1927117488709942|0.8732552715311255|     NULL|\n",
      "|    min|              3.8|               0.08|                0.0|               0.6|              0.009|                1.0|                 6.0|             0.98711|               2.72|               0.22|               8.0|                 3|      red|\n",
      "|    max|             15.9|               1.58|               1.66|              65.8|              0.611|              289.0|               440.0|             1.03898|               4.01|                2.0|              14.9|                 9|    white|\n",
      "+-------+-----------------+-------------------+-------------------+------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wine_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2c825-cd9a-48d1-b45b-d5310b75254e",
   "metadata": {},
   "source": [
    "3. Checking for Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85ab2bf6-b450-44a8-b253-6a62a9f71af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+---+---------+-------+-------+---------+\n",
      "|fixed acidity|volatile acidity|citric acid|residual sugar|chlorides|free sulfur dioxide|total sulfur dioxide|density| pH|sulphates|alcohol|quality|wine_type|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+---+---------+-------+-------+---------+\n",
      "|            0|               0|          0|             0|        0|                  0|                   0|      0|  0|        0|      0|      0|        0|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+---+---------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count, isnan\n",
    "\n",
    "numeric_cols = [c for c, t in wine_df.dtypes if t in (\"double\", \"int\", \"float\")]\n",
    "string_cols = [c for c, t in wine_df.dtypes if t not in (\"double\", \"int\", \"float\")]\n",
    "\n",
    "missing = wine_df.select(\n",
    "    # numeric columns — check NULL or NaN\n",
    "    *[\n",
    "        count(when(col(c).isNull() | isnan(c), c)).alias(c)\n",
    "        for c in numeric_cols\n",
    "    ],\n",
    "    # string columns — only check NULL\n",
    "    *[\n",
    "        count(when(col(c).isNull(), c)).alias(c)\n",
    "        for c in string_cols\n",
    "    ]\n",
    ")\n",
    "\n",
    "missing.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4112f-c8c7-441f-9948-1f11525ea21f",
   "metadata": {},
   "source": [
    "**SparkSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c61085-a716-48e9-b4bd-f59d8039ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.createOrReplaceTempView(\"wines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa002c-5f03-49e2-b607-2c9f65a4180d",
   "metadata": {},
   "source": [
    "1. Average alcohol content by wine type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b02c03e-d18b-4a26-b5f1-62e640d1723a",
   "metadata": {},
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT wine_type, AVG(alcohol) AS avg_alcohol\n",
    "    FROM wines\n",
    "    GROUP BY wine_type\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4062c645-78db-4c03-94d3-809c6323fc9f",
   "metadata": {},
   "source": [
    "2. Count wines by quality rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66af5cd-336a-42ba-bf83-b5fa642dffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|quality|count|\n",
      "+-------+-----+\n",
      "|      3|   30|\n",
      "|      4|  216|\n",
      "|      5| 2138|\n",
      "|      6| 2836|\n",
      "|      7| 1079|\n",
      "|      8|  193|\n",
      "|      9|    5|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT quality, COUNT(*) AS count\n",
    "    FROM wines\n",
    "    GROUP BY quality\n",
    "    ORDER BY quality\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194de780-d7a0-4e96-a52c-3462b628bdeb",
   "metadata": {},
   "source": [
    "3. Highest alcohol red wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6856804a-4a2a-46a9-bb2e-9e01e108ab49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+---------+\n",
      "|fixed acidity|volatile acidity|citric acid|residual sugar|chlorides|free sulfur dioxide|total sulfur dioxide|density|  pH|sulphates|alcohol|quality|wine_type|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+---------+\n",
      "|         15.9|            0.36|       0.65|           7.5|    0.096|               22.0|                71.0| 0.9976|2.98|     0.84|   14.9|      5|      red|\n",
      "|          5.2|            0.34|        0.0|           1.8|     0.05|               27.0|                63.0| 0.9916|3.68|     0.79|   14.0|      6|      red|\n",
      "|          5.0|            0.42|       0.24|           2.0|     0.06|               19.0|                50.0| 0.9917|3.72|     0.74|   14.0|      8|      red|\n",
      "|          4.9|            0.42|        0.0|           2.1|    0.048|               16.0|                42.0|0.99154|3.71|     0.74|   14.0|      7|      red|\n",
      "|          5.2|            0.34|        0.0|           1.8|     0.05|               27.0|                63.0| 0.9916|3.68|     0.79|   14.0|      6|      red|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM wines\n",
    "    WHERE wine_type = 'red'\n",
    "    ORDER BY alcohol DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe1c6d-15b1-40f2-a3a1-28a294aae289",
   "metadata": {},
   "source": [
    "4. Correlation: alcohol vs quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c82272db-4cfe-401d-b6ee-789851c640c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|alcohol_quality_corr|\n",
      "+--------------------+\n",
      "|  0.4443185200076535|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT corr(alcohol, quality) AS alcohol_quality_corr\n",
    "    FROM wines;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a4cbfe4-c3d1-40a2-a4b2-54a48dfc1685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- ChestPainType: string (nullable = true)\n",
      " |-- RestingBP: integer (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- FastingBS: integer (nullable = true)\n",
      " |-- RestingECG: string (nullable = true)\n",
      " |-- MaxHR: integer (nullable = true)\n",
      " |-- ExerciseAngina: string (nullable = true)\n",
      " |-- Oldpeak: double (nullable = true)\n",
      " |-- ST_Slope: string (nullable = true)\n",
      " |-- HeartDisease: integer (nullable = true)\n",
      "\n",
      "+---+-----------+\n",
      "|Age|Cholesterol|\n",
      "+---+-----------+\n",
      "| 54|        195|\n",
      "| 54|        208|\n",
      "| 58|        164|\n",
      "| 54|        273|\n",
      "| 60|        248|\n",
      "| 53|        260|\n",
      "| 52|        284|\n",
      "| 53|        468|\n",
      "| 51|        188|\n",
      "| 53|        518|\n",
      "| 56|        167|\n",
      "| 54|        224|\n",
      "| 65|        306|\n",
      "| 54|        230|\n",
      "| 54|        294|\n",
      "| 52|        259|\n",
      "| 59|        318|\n",
      "| 52|        180|\n",
      "| 51|        194|\n",
      "| 58|        213|\n",
      "+---+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "heart = spark.read.csv(\"heart.csv\", header=True, inferSchema=True, sep=',')\n",
    "heart.createOrReplaceTempView(\"heart\")\n",
    "heart.printSchema()\n",
    "spark.sql(\"SELECT Age, Cholesterol FROM heart WHERE Age > 50\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5f7adfa-56ec-4433-984d-3d788af946a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+\n",
      "|Age|Sex|HeartDisease|\n",
      "+---+---+------------+\n",
      "| 40|  M|           0|\n",
      "| 49|  F|           1|\n",
      "| 37|  M|           0|\n",
      "| 48|  F|           1|\n",
      "| 54|  M|           0|\n",
      "+---+---+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#Select with dataframe\n",
    "\n",
    "heart.select(\"Age\", \"Sex\", \"HeartDisease\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22f159d9-bea2-4bcd-8189-0dc2f5d81b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-12-11 12:25:11.249\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[TABLE_OR_VIEW_NOT_FOUND] The table or view `cvd` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01\", \"context\": {\"errorClass\": \"TABLE_OR_VIEW_NOT_FOUND\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o29.sql.\\n: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `cvd` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 3 pos 9;\\n'GlobalLimit 5\\n+- 'LocalLimit 5\\n   +- 'Project ['Age, 'Sex, 'HeartDisease]\\n      +- 'UnresolvedRelation [cvd], [], false\\n\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:306)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\\n\\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:462)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:449)\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:467)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:833)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:306)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 22 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/Users/ninazorawska/Desktop/~bda/ai_course_env/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/Users/ninazorawska/Desktop/~bda/ai_course_env/lib/python3.13/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `cvd` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 3 pos 9;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Project ['Age, 'Sex, 'HeartDisease]\n      +- 'UnresolvedRelation [cvd], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Select with Spark SQL\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[33;43m    SELECT Age, Sex, HeartDisease\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43m    FROM cvd\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43m    LIMIT 5\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m.show()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Both give the same output; SQL is concise for analysts, DSL is good for Python developers.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/~bda/ai_course_env/lib/python3.13/site-packages/pyspark/sql/session.py:1810\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1806\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1807\u001b[39m             errorClass=\u001b[33m\"\u001b[39m\u001b[33mINVALID_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1808\u001b[39m             messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(args).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1809\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1810\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1811\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1812\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/~bda/ai_course_env/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/~bda/ai_course_env/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `cvd` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 3 pos 9;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Project ['Age, 'Sex, 'HeartDisease]\n      +- 'UnresolvedRelation [cvd], [], false\n"
     ]
    }
   ],
   "source": [
    "#Select with Spark SQL\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Age, Sex, HeartDisease\n",
    "    FROM cvd\n",
    "    LIMIT 5\n",
    "\"\"\").show()\n",
    "\n",
    "# Both give the same output; SQL is concise for analysts, DSL is good for Python developers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245c4ba-5498-435d-a306-9ed0604ea5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering with dataframes\n",
    "\n",
    "cvd_df.filter(cvd_df.Age > 50).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf96f21-e6c2-4e11-81db-e2648bd42615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering with Spark SQL\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM cvd\n",
    "    WHERE Age > 50\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c6b40-c089-45fe-a123-4e710e83228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation with DataFrames\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "cvd_df.groupBy(\"Sex\").agg(avg(\"MaxHR\").alias(\"avg_maxhr\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe073c6-781c-4316-a278-8e418c798f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation with Spark SQL\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Sex, AVG(MaxHR) AS avg_maxhr\n",
    "    FROM cvd\n",
    "    GROUP BY Sex\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d1611-8659-4c83-98d7-b14c5438fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Join: Find pairs of patients who share the same chest pain type with Spark SQL\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT a.Age AS Age1, b.Age AS Age2, a.ChestPainType\n",
    "    FROM cvd a\n",
    "    JOIN cvd b ON a.ChestPainType = b.ChestPainType\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_course_env)",
   "language": "python",
   "name": "ai_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
